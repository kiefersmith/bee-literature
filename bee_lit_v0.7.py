import osimport timeimport picklefrom tqdm import tqdmimport sysfrom optparse import OptionParserimport xml.etree.ElementTree as etfrom lxml import etreefrom bs4 import BeautifulSoupfrom nltk.tokenize import sent_tokenize, word_tokenizefrom nltk.util import ngramsfrom nltk.corpus import stopwordsfrom nltk.stem.wordnet import WordNetLemmatizerfrom nltk.stem.porter import PorterStemmerimport nltkimport pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport refrom collections import Counterfrom itertools import compressfrom sklearn.feature_extraction.text import TfidfVectorizerdef load_and_setup():    global stop    global lemma    file = open('/Users/kiefer/Downloads/SearchResults-1-Clean/SearchResults.xml')    page = file.read()    soup = BeautifulSoup(page, 'lxml')    records = soup.find_all('record')    stop = list(stopwords.words('english'))    other_stop = ['(English)', 'copy', 'written', 'users', 'suggest', 'site', 'print', 'download', 'listserv', 'abstracts', 'warranty', 'email', 'emailed', 'article', 'publish', 'published','however','study', 'summary', 'chapter', 'publisher', 'mellifera', 'apis', 'author', 'ltd', 'company', 'publishing', 'honeybee', 'bee', 'ha', 'also', 'taylor', 'francis', 'springer', 'oxford', 'press', 'usa', 'oxford press', 'copyright', 'john', 'gruyter', 'elsevier', 'abstract:', 'description:', 'journal', 'abstract', 'university', 'science', 'quot', 'wiley', 'amp', 'apos','none', 'article', 'journal']    for word in other_stop:        stop.append(word)    lemma = WordNetLemmatizer()    porter = PorterStemmer()    return records, lemma, porterdef process_text(records):    abs_dict = []    title_dict = []    year_dict = []    journal_dict = []    word_count = []    language = []    sep = '[ABSTRACT FROM AUTHOR]'    for rec in records:        title_dict.append(rec.title.text)        if rec.abstract is None:            abs_dict.append('')        else:            abstract = (rec.abstract.text.split(sep))[0]            abs_dict.append(abstract)        if rec.year is None:            year_dict.append(0)        else:            year = int(rec.year.text)            year_dict.append(year)        if rec.periodical is None:            journal_dict.append('None')        else:            journal_dict.append(rec.periodical.text)        if rec.language is None:            language.append('None')        else:            language.append(rec.language.text)    return abs_dict, title_dict, year_dict, journal_dict, word_count, languageclass Tokenize:    def tokenize_and_filter(text):        if 'honey bee' in text:            text = text.replace('honey bee', 'honeybee')        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]        # word_count.append(len(tokens))        filtered_tokens = []        for token in tokens:            if token not in stop:                if len(token) > 3:                    if re.search('^[a-zA-Z]*$', token):                        filtered_tokens.append(lemma.lemmatize(token))            else:                pass        return filtered_tokens    def tokenize_and_filter_bigrams(text, bg):        if 'honey bee' in text:            text = text.replace('honey bee', 'honeybee')        else:            pass        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]        # word_count.append(len(tokens))        filtered_tokens = []        for token in tokens:            if token not in stop:                if len(token) > 3:                    if re.search('^[a-zA-Z]*$', token):                        l = lemma.lemmatize(token)                        filtered_tokens.append(l)                else:                    pass            else:                pass        bigrams = []        if bg is True:            bgs = list(ngrams(filtered_tokens, 2))            for g in bgs:                bigrams.append(' '.join(g))        else:            pass        return filtered_tokens, bigrams    def tokenize_and_filter_porter(text):        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]        # word_count.append(len(tokens))        filtered_tokens = []        for token in tokens:            if token not in stop:                if len(token) > 3:                    if re.search('^[a-zA-Z]*$', token):                        filtered_tokens.append(porter.stem(token))        return(filtered_tokens)    def tokenize_and_filter_bigrams_porter(text, bg):        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]        # word_count.append(len(tokens))        filtered_tokens = []        for token in tokens:            if token not in stop:                if len(token) > 3:                    if re.search('^[a-zA-Z]*$', token):                        l = porter.stem(token)                        filtered_tokens.append(l)                else:                    pass        bigrams = []        if bg is True:            bgs = list(ngrams(filtered_tokens, 2))            for g in bgs:                bigrams.append(' '.join(g))        else:            pass        return filtered_tokens, bigramsdef filter_years(start, end, year_dict, abs_dict):    yd = pd.DataFrame(year_dict)    #unique = yd[0].unique()    #unique.sort()    vals = yd[0].value_counts()    vals.sort_index(inplace=True)    if start is not None and end is not None:        index1 = yd[0] <= end        index2 = yd[0] >= start        index = np.logical_and(index1, index2)    new_dict = np.array(abs_dict)[np.array(index)]    return(new_dict, index)def make_year_list(year_dict_filtered, abs_dict_filtered, journal_dict_filtered):    journals_by_year = []    first, i = filter_years(start=1917,                                     end=1949,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    second, i = filter_years(start=1950,                                     end=1959,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    third, i = filter_years(start=1960,                                     end=1969,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    fourth, i = filter_years(start=1970,                                     end=1979,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    fifth, i = filter_years(start=1980,                                     end=1989,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    sixth, i = filter_years(start=1990,                                     end=1999,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    seventh, i = filter_years(start=2000,                                     end=2004,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    eighth, i = filter_years(start=2005,                                     end=2009,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    ninth, i = filter_years(start=2010,                                     end=2014,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    tenth, i = filter_years(start=2015,                                     end=2016,                                     year_dict=year_dict_filtered,                                     abs_dict=abs_dict_filtered)    journals_by_year.append(np.array(journal_dict_filtered)[i])    year_list = [first, second, third, fourth, fifth, sixth, seventh, eighth, ninth, tenth]    return year_list, journals_by_yeardef count(words, bg):    if bg is True:        index = []        for x in words:            for y in x:                z = y.split()                if ((z[0].lower() in stop) + (z[1].lower() in stop)) != 0:                    index.append(False)                else:                    index.append(True)        flat_list = [item for sublist in words for item in sublist]        flat_list = np.array(flat_list)[np.array(index)]    else:        flat_list = [item for sublist in words for item in sublist]    fl = Counter(flat_list)    fl_keys = list(fl.keys())    fl_v = list(fl.values())    freq = pd.DataFrame({'word':fl_keys, 'number':fl_v})    freq = freq.sort_values(by='number', ascending=False)    if bg is False:        word_counts = freq[~freq.word.str.lower().isin(stop)]    else:        word_counts = freq    return(word_counts)def get_word_counts(word_list, title_list, i):    words = []    bigrams = []    for x in word_list:        t, bg = Tokenize.tokenize_and_filter_bigrams(text=x, bg=True)        words.append(t)        bigrams.append(bg)    word_counts = count(words=words, bg=False)    bigram_counts = count(words=bigrams, bg=True)    x = np.arange(20)    fig, ax = plt.subplots()    plt.subplots_adjust(left = .2)    plt.barh(x, word_counts.number[0:20])    plt.yticks(x, (word_counts.word[0:20]))    plt.title('Word Counts' + ' ' + title_list[i])    plt.savefig('/Users/kiefer/Desktop/figures/'+ title_list[i]+'WC')    x = np.arange(10)    fig, ax = plt.subplots()    plt.subplots_adjust(left = .35)    plt.barh(x, bigram_counts.number[0:10])    plt.yticks(x, (bigram_counts.word[0:10]))    plt.title('Bigram Counts' + ' ' + title_list[i])    plt.savefig('/Users/kiefer/Desktop/figures/'+ title_list[i]+'BGC')    word_counts[0:29].to_csv('/Users/kiefer/Desktop/figures/'+ title_list[i]+'WC.csv')    bigram_counts[0:29].to_csv('/Users/kiefer/Desktop/figures/'+ title_list[i]+'BGC.csv')def replace_journal_title(journal_dict, replace_dict):    for i in range(0, len(replace_dict)):        journal_dict = [list(replace_dict.values())[i] if w == list(replace_dict.keys())[i] else w for w in journal_dict]    return(journal_dict)def filter_by_journal(filter_list, journal_dict_filtered, abs_dict_filtered):    index=[]    for x in journal_dict_filtered:        if x in filter_list:            index.append(True)        else:            index.append(False)    filter_abstracts=np.array(abs_dict_filtered)[index]    return(filter_abstracts, index)def get_tf_idf(word_list):    vectorizer = TfidfVectorizer(min_df=.1,                                ngram_range=(1,2),                                stop_words=stop,                                tokenizer=Tokenize.tokenize_and_filter,                                use_idf=True,                                sublinear_tf=True)    tfidf = vectorizer.fit_transform(word_list)    t = vectorizer.get_feature_names()    order = tfidf.sum(axis=0).argsort().tolist()    w = []    for i in range(0,20):        w.append(t[order[0][i]])    return(w)def main():    t = time.time()    print('Loading...')    records, lemma, porter = load_and_setup()    abs_dict = (pickle.load(open('abs_dict.pkl', 'rb')))    journal_dict = (pickle.load(open('journal_dict.pkl', 'rb')))    year_dict = (pickle.load(open('year_dict.pkl', 'rb')))    print('Filtering text...')    filtered_dict, index = filter_years(start=1917,                                        end=2016,                                        year_dict=year_dict,                                        abs_dict=abs_dict)    # replace truncated titles    replace_dict = {'J Econ Entomol':'Journal of Economic Entomology',                    'J Insect Physiol':'Journal of Insect Physiology',                    'J Invertebr Pathol':'Journal of Invertebrate Pathology',                    'J Exp Biol':'Journal of Experimental Biology',                    'Journal apic Res':'Journal of Apicultural Research',                    'Insect Mol Biol':'Insect Molecular Biology',                    'Proc Natl Acad Sci U S A':'Proceedings of the National Academy of Sciences of the United States of America',                    'Insect Biochem Mol Biol':'Insect Biochemistry and Molecular Biology',                    'J Comp Physiol A Neuroethol Sens Neural Behav Physiol':'Journal of Comparative Physiology A',                    'J Chem Ecol':'Journal of Chemical Ecology',                    'Exp Appl Acarol':'Experimental and Applied Acarology',                    'Sci Rep':'Scientific Reports',                    'J Agric Food Chem':'Journal of Agricultural Food Chemistry',                    'Genet Mol Res':'Genetic Molecular Research',                    'J Insect Sci':'Journal of Insect Science',                    'Pest Manag Sci':'Pest Management Science',                    'Proc Biol Sci':'Proceedings of the Royal Society B',                    'J Comp Neurol':'Journal of Comparative Neurology',                    'Mol Ecol':'Molecular Ecology',                    'J Allergy Clin Immunol':'Journal of Allergy and Clinical Immunology',                    'Arch Insect Biochem Physiol':'Archives of Insect Biochemistry and Physiology',                    'Journal Comp Physiol':'Journal of Comparative Physiology'}    # creates an index of unique abstracts    seen = set()    unique = []    for x in filtered_dict:        if x not in seen:            unique.append(True)            seen.add(x)        else:            unique.append(False)    journal_dict_filtered = np.array(journal_dict)[index][unique]    journal_dict_filtered = replace_journal_title(journal_dict_filtered, replace_dict)    year_dict_filtered = np.array(year_dict)[index][unique]    abs_dict_filtered = np.array(filtered_dict)[unique]    jd = pd.DataFrame(list(journal_dict_filtered))    jd = jd[0].value_counts()    jd_list = pd.DataFrame(list(jd.index[1:101]))    # plot journal freq    fig, ax = plt.subplots()    plt.subplots_adjust(left=.5)    plt.barh(np.arange(25), jd[1:26])    plt.yticks(np.arange(25), (jd.index[1:26]))    for i, v in zip(np.arange(25), jd[1:26]):        ax.text(v+3, i-.25, str(v))    gen_sci = ['PLoS One','Journal of Experimental Biology','Journal of Theoretical Biology','Vision Research','Brain Research','The Lancet','Science','New Scientist','Nature','Proceedings of the National Academy of Sciences of the United States of America','Current Biology','Peptides','Tissue and Cell','Cell','Naturwissenschaften']    bio = ['Current Opinion in Neurobiology', 'Neuron', 'Neuroscience Letters', 'Trends in Neurosciences', 'Progress in Neurobiology', 'Journal of Insect Physiology', 'Advances in Insect Physiology', 'Journal of Comparative Physiology A Sensory Neural and Behavioral Physiology', 'Sensory Neural and Behavioral Physiology', 'General and Comparative Endocrinology', 'Journal of Comparative Physiology A', 'Journal of Comparative Physiology', 'Gene', 'BMC Genomics', 'Molecular Phylogenetics and Evolution', 'Developmental Biology']    mol_bio = ['Insect Biochemistry and Molecular Biology','Comparative Biochemistry and Physiology Part B: Comparative Biochemistry','Insect Biochemistry','Comparative Biochemistry and Physiology Part B: Biochemistry and Molecular Biology','Comparative Biochemistry and Physiology Part A: Physiology','Biochemical and Biophysical Research Communications','Comparative Biochemistry and Physiology Part C: Comparative Pharmacology','Biochimica et Biophysica Acta (BBA) - Biomembranes','Comparative Biochemistry and Physiology Part A: Molecular &amp; Integrative Physiology','Biophysical Journal','Comparative Biochemistry and Physiology','Insect Molecular Biology','Journal of Molecular Biology','Journal of Chromatography A','FEBS Letters','Chemosphere','Talanta','Micron']    entomol = ['Annals of the Entomological Society of America','Journal of Asia-Pacific Entomology','Journal of the Kansas Entomological Society','Experimental and Applied Acarology','Arthropod Structure &amp; Development','Experimental and Applied Acarology','Current Opinion in Insect Science','Verhandlungen der Deutschen Zoologischen Gesellschaft']    socio = ['Animal Behaviour','Behavioral Ecology and Sociobiology','Trends in Ecology &amp; Evolution','Insectes Sociaux','Journal of Insect Behavior','Behavioral Ecology &amp; Sociobiology','Sociobiology','Behavioural Processes','Advances in the Study of Behavior']    bees = ['Apidologie','Journal of Apicultural Research','Bee World','Indian Bee Journal','Journal of Apicultural Science','Honeybee Science','Apicoltore Moderno']    health = ['Journal of Ethnopharmacology','Journal of Allergy and Clinical Immunology','Annals of Allergy, Asthma &amp; Immunology','Fish &amp; Shellfish Immunology','Revue Fran√ßaise d&apos;Allergologie et d&apos;Immunologie Clinique','Developmental & Comparative Immunology','Toxicon','Food and Chemical Toxicology','Journal of Invertebrate Pathology','Experimental Gerontology']    food = ['Crop Protection','Scientia Horticulturae','Agriculture, Ecosystems &amp; Environment','Pesticide Biochemistry and Physiology','Food Chemistry','Biological Control']    envir = ['Biological Conservation','Environmental Entomology','Journal of Chemical Ecology','Science of The Total Environment','Ecological Modelling','Flora - Morphology, Distribution, Functional Ecology of Plants','South African Journal of Botany']    econ = ['Ecological Economics','Journal of Economic Entomology']    comp = ['Applied Soft Computing','International Journal of Electrical Power & Energy Systems','Expert Systems with Applications','Information Sciences']    j_list = [gen_sci, bio, mol_bio, entomol, socio, bees, health, food, envir, econ, comp]    decades = ['Pre-50s', '1950s', '1960s', '1970s', '1980s','1990s', '2000-2004', '2005-2009', '2010-2014', '2015 and 2016']    categories = ['General Science', 'Biological Sciences and Morphology', 'Molecular and Biochemistry', 'Entomology', 'Sociobiology', 'Bees', 'Health', ' Food and Agriculture', 'Environmental and Ecology', 'Economics', 'Computing']    # filter by journal    # year plot    years = pd.DataFrame(year_dict_filtered)    fig, ax = plt.subplots()    years[0].value_counts().iloc[years[0].value_counts().index.argsort()].plot(ax=ax)    years[0].value_counts().iloc[years[0].value_counts().index.argsort()].plot(kind='bar', color='r', sharex=True, sharey=True)    # get words year    # Do I have to filter this by top journals as well???    l, journals_by_year = make_year_list(year_dict_filtered, abs_dict_filtered, journal_dict_filtered)    for x in range(len(journals_by_year)):        frame = pd.DataFrame(journals_by_year[x])        vc = frame[0].value_counts()        vc.to_csv('/Users/kiefer/Desktop/figures/Journal Counts ' + decades[x] + '.csv')    for x in range(0, 10):        wl = list(set(l[x]))        get_word_counts(wl, decades, x)        try:            w = get_tf_idf(wl)        except ValueError:            print('Corpus not diverse enough.')        pd.DataFrame(w).to_csv('/Users/kiefer/Desktop/figures/tfidf_years/' + decades[x] + '.csv')    # By journal category.    for j in range(len(j_list)):        word_list, index = filter_by_journal(j_list[j], journal_dict_filtered, abs_dict_filtered)        frame = pd.DataFrame(np.array(journal_dict_filtered)[index])        vc = frame[0].value_counts()        vc.to_csv('/Users/kiefer/Desktop/figures/Journal Counts ' + categories[j] + '.csv')        get_word_counts(word_list=word_list, title_list=categories, i=j)        w = get_tf_idf(word_list)        pd.DataFrame(w).to_csv('/Users/kiefer/Desktop/figures/tfidf/' + categories[j] + '.csv')    for y in range(len(l)):        for j in range(len(j_list)):                    title = (decades[y] + ' ' + categories[j])                    word_list, index = filter_by_journal(j_list[j], list(journals_by_year[y]), l[y])                    try:                        get_word_counts(word_list=word_list, title_list=[title], i=0)                    except AttributeError:                        print('No journals/ abstracts in this time period:' + title)    get_word_counts(abs_dict_filtered, ['Full Corpus'], i=0)    w = get_tf_idf(abs_dict_filtered)    pd.DataFrame(w).to_csv('/Users/kiefer/Desktop/figures/tfidf/Full Corpus.csv')    print(time.time() - t)# plt.ioff() to turn off interactive plotting.    if __name__ == '__main__':    main()